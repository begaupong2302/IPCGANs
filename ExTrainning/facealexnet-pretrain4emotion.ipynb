{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10333537,"sourceType":"datasetVersion","datasetId":6398407}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.utils.data as data\nfrom PIL import Image\nimport os\nimport numpy as np\nimport torchvision\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:35:52.107303Z","iopub.execute_input":"2024-12-30T11:35:52.107568Z","iopub.status.idle":"2024-12-30T11:35:56.093291Z","shell.execute_reply.started":"2024-12-30T11:35:52.107546Z","shell.execute_reply":"2024-12-30T11:35:56.092409Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef check_dir(dir):\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\nclass Img_to_zero_center(object):\n    def __int__(self):\n        pass\n    def __call__(self, t_img):\n        '''\n        :param img:tensor be 0-1\n        :return:\n        '''\n        t_img=(t_img-0.5)*2\n        return t_img\n\nclass Reverse_zero_center(object):\n    def __init__(self):\n        pass\n    def __call__(self,t_img):\n        t_img=t_img/2+0.5\n        return t_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:35:56.094159Z","iopub.execute_input":"2024-12-30T11:35:56.094640Z","iopub.status.idle":"2024-12-30T11:35:56.100052Z","shell.execute_reply.started":"2024-12-30T11:35:56.094609Z","shell.execute_reply":"2024-12-30T11:35:56.099149Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CACD(data.Dataset):\n    def __init__(self, split='train', transforms=None, label_transforms=None):\n        list_root = \"/kaggle/input/facial-expression-recognition/lists\"\n        data_root = \"/kaggle/input/facial-expression-recognition/FER\"\n        list_emotion = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n        if split == \"train\":\n            self.list_path= os.path.join(list_root,\"train.txt\")\n        else:\n            self.list_path = os.path.join(list_root, \"test.txt\")\n        self.images_labels=[]#path\n        self.transform=transforms\n        self.label_transform=label_transforms\n        with open(self.list_path) as fr:\n            lines=fr.readlines()\n            for line in lines:\n                line.strip()\n                item=line.split()\n                image_label=[]\n                image_label.append(os.path.join(data_root,list_emotion[int(item[1])],item[0]))\n                try:\n                    image_label.append(np.array(int(item[1])))\n                except:\n                  print(item[0])\n                  print(item[1])\n                self.images_labels.append(image_label)\n\n    def __getitem__(self, idx):\n        img_path,label=self.images_labels[idx]\n        img=Image.open(img_path)\n\n        if self.transform is not None:\n            img=self.transform(img)\n        if self.label_transform is None:\n            label=torch.from_numpy(label)\n        return img,label\n\n    def __len__(self):\n        return len(self.images_labels)\n\n\ntransforms = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1., 1., 1.]),\n])\n\nCACD_dataset=CACD(\"train\", transforms, None)\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=CACD_dataset,\n    batch_size=32,\n    shuffle=False\n)\nfor idx,(img,label) in enumerate(train_loader):\n    print(img.size())\n    print(label.size())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:35:56.100825Z","iopub.execute_input":"2024-12-30T11:35:56.101133Z","iopub.status.idle":"2024-12-30T11:35:56.555329Z","shell.execute_reply.started":"2024-12-30T11:35:56.101094Z","shell.execute_reply":"2024-12-30T11:35:56.554495Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 3, 400, 400])\ntorch.Size([32])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import math\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.functional import pad\nfrom torch.nn.modules import Module\nfrom torch.nn.modules.utils import _single, _pair, _triple\n\nclass _ConvNd(Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, transposed, output_padding, groups, bias):\n        super(_ConvNd, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.transposed = transposed\n        self.output_padding = output_padding\n        self.groups = groups\n        if transposed:\n            self.weight = Parameter(torch.Tensor(\n                in_channels, out_channels // groups, *kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(\n                out_channels, in_channels // groups, *kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def __repr__(self):\n        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'\n             ', stride={stride}')\n        if self.padding != (0,) * len(self.padding):\n            s += ', padding={padding}'\n        if self.dilation != (1,) * len(self.dilation):\n            s += ', dilation={dilation}'\n        if self.output_padding != (0,) * len(self.output_padding):\n            s += ', output_padding={output_padding}'\n        if self.groups != 1:\n            s += ', groups={groups}'\n        if self.bias is None:\n            s += ', bias=False'\n        s += ')'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\nclass Conv2d(_ConvNd):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(Conv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n\n    def forward(self, input):\n        return _conv2d_same_padding(input, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n# custom conv2d, because pytorch don't have \"padding='same'\" option.\ndef _conv2d_same_padding(input, weight, bias=None, stride=(1,1), padding=1, dilation=(1,1), groups=1):\n\n    input_rows = input.size(2)\n    filter_rows = weight.size(2)\n    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n    out_rows = (input_rows + stride[0] - 1) // stride[0]\n    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n                  input_rows)\n    padding_rows = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    rows_odd = (padding_rows % 2 != 0)\n    padding_cols = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    cols_odd = (padding_rows % 2 != 0)\n\n    if rows_odd or cols_odd:\n        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n\n    return F.conv2d(input, weight, bias, stride,\n                  padding=(padding_rows // 2, padding_cols // 2),\n                  dilation=dilation, groups=groups)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:35:56.557448Z","iopub.execute_input":"2024-12-30T11:35:56.557672Z","iopub.status.idle":"2024-12-30T11:35:56.687118Z","shell.execute_reply.started":"2024-12-30T11:35:56.557653Z","shell.execute_reply":"2024-12-30T11:35:56.686295Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AgeAlexNet(nn.Module):\n    def __init__(self,pretrainded=False,modelpath=None):\n        super(AgeAlexNet, self).__init__()\n        assert pretrainded is False or modelpath is not None,\"pretrain model need to be specified\"\n        self.features = nn.Sequential(\n            Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.LocalResponseNorm(2,2e-5,0.75),\n\n            Conv2d(96, 256, kernel_size=5, stride=1,groups=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.LocalResponseNorm(2, 2e-5, 0.75),\n\n            Conv2d(256, 384, kernel_size=3, stride=1),\n            nn.ReLU(inplace=True),\n\n            Conv2d(384, 384, kernel_size=3,stride=1,groups=2),\n            nn.ReLU(inplace=True),\n\n            Conv2d(384, 256, kernel_size=3,stride=1,groups=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.age_classifier=nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 8),\n        )\n        if pretrainded is True:\n            self.load_pretrained_params(modelpath)\n\n        self.Conv3_feature_module=nn.Sequential()\n        self.Conv4_feature_module=nn.Sequential()\n        self.Conv5_feature_module=nn.Sequential()\n        self.Pool5_feature_module=nn.Sequential()\n        for x in range(10):\n            self.Conv3_feature_module.add_module(str(x), self.features[x])\n        for x in range(10,12):\n            self.Conv4_feature_module.add_module(str(x),self.features[x])\n        for x in range(12,14):\n            self.Conv5_feature_module.add_module(str(x),self.features[x])\n        for x in range(14,15):\n            self.Pool5_feature_module.add_module(str(x),self.features[x])\n\n\n    def forward(self, x):\n        self.conv3_feature=self.Conv3_feature_module(x)\n        self.conv4_feature=self.Conv4_feature_module(self.conv3_feature)\n        self.conv5_feature=self.Conv5_feature_module(self.conv4_feature)\n        pool5_feature=self.Pool5_feature_module(self.conv5_feature)\n        self.pool5_feature=pool5_feature\n        flattened = pool5_feature.view(pool5_feature.size(0), -1)\n        age_logit = self.age_classifier(flattened)\n        return age_logit\n\n    def load_pretrained_params(self,path):\n        # step1: load pretrained model\n        pretrained_dict = torch.load(path)\n        # step2: get model state_dict\n        model_dict = self.state_dict()\n        # step3: remove pretrained_dict params which is not in model_dict\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # step4: update model_dict using pretrained_dict\n        model_dict.update(pretrained_dict)\n        # step5: update model using model_dict\n        self.load_state_dict(model_dict)\n\n\nclass AgeClassify:\n    def __init__(self, modelz = AgeAlexNet(pretrainded=False).cuda()):\n        #step 1:define model\n        self.model=modelz\n        #step 2:define optimizer\n        self.optim=torch.optim.Adam(self.model.parameters(),lr=1e-4,betas=(0.5, 0.999))\n        #step 3:define loss\n        self.criterion=nn.CrossEntropyLoss().cuda()\n\n    def train(self,input,label):\n        self.model.train()\n        output=self.model(input)\n        self.loss=self.criterion(output,label)\n\n    def val(self,input):\n        self.model.eval()\n        output=F.softmax(self.model(input),dim=1).max(1)[1]\n        return output\n\n    def save_model(self,dir,filename):\n        torch.save(self.model.state_dict(),os.path.join(dir,filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:41:36.550370Z","iopub.execute_input":"2024-12-30T12:41:36.550612Z","iopub.status.idle":"2024-12-30T12:41:37.056014Z","shell.execute_reply.started":"2024-12-30T12:41:36.550589Z","shell.execute_reply":"2024-12-30T12:41:37.054967Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"batch_size 512 20 epochs","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\n\n#define\nmax_epoches = 20\nbatch_size = 256\nsave_interval = 20000\nval_interval = 20000\nsaved_model_folder = '/kaggle/working/checkpoint'\ncheck_dir(saved_model_folder)\n\ntransforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((227, 227)),\n    torchvision.transforms.ToTensor(),\n    Img_to_zero_center()\n])\n\ntrain_dataset = CACD(\"train\", transforms, None)\ntest_dataset = CACD(\"test\", transforms, None)\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nmodel=AgeClassify()\noptim=model.optim\n\nfor epoch in range(max_epoches):\n    for train_idx, (img,label) in enumerate(train_loader):\n        img=img.cuda()\n        label=label.cuda()\n\n        #train\n        optim.zero_grad()\n        model.train(img,label)\n        loss=model.loss\n        loss.backward()\n        optim.step()\n\n        if (train_idx + 1)%50 == 0:\n            print('step %d/%d, cls_loss = %.3f' % (train_idx, len(train_loader), loss))\n\n        # save the parameters at the end of each save interval\n        if train_idx*batch_size % save_interval == 0:\n            model.save_model(dir=saved_model_folder,\n                              filename='epoch_%d_iter_%d.pth'%(epoch, train_idx))\n            print('checkpoint has been created!')\n\n        #val step\n\n        if train_idx % val_interval == 0:\n            train_correct=0\n            train_total=0\n            with torch.no_grad():\n                for val_img,val_label in tqdm(test_loader):\n                    val_img=val_img.cuda()\n                    val_label=val_label.cuda()\n                    output=model.val(val_img)\n                    train_correct += (output == val_label).sum()\n                    train_total += val_img.size()[0]\n\n            print('validate has been finished!')\n            print('val_acc = %.3f' % (train_correct.cpu().numpy()/train_total))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:39:43.647648Z","iopub.execute_input":"2024-12-30T11:39:43.647969Z","iopub.status.idle":"2024-12-30T12:41:36.549122Z","shell.execute_reply.started":"2024-12-30T11:39:43.647939Z","shell.execute_reply":"2024-12-30T12:41:36.548218Z"}},"outputs":[{"name":"stdout","text":"checkpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:16<00:00,  1.48s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.173\nstep 49/101, cls_loss = 2.067\nstep 99/101, cls_loss = 1.866\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.249\nstep 49/101, cls_loss = 1.779\nstep 99/101, cls_loss = 1.751\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.362\nstep 49/101, cls_loss = 1.460\nstep 99/101, cls_loss = 1.458\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.471\nstep 49/101, cls_loss = 1.323\nstep 99/101, cls_loss = 1.226\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.495\nstep 49/101, cls_loss = 1.339\nstep 99/101, cls_loss = 1.274\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.522\nstep 49/101, cls_loss = 1.249\nstep 99/101, cls_loss = 1.271\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.545\nstep 49/101, cls_loss = 1.091\nstep 99/101, cls_loss = 1.254\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.571\nstep 49/101, cls_loss = 1.200\nstep 99/101, cls_loss = 1.095\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.603\nstep 49/101, cls_loss = 1.090\nstep 99/101, cls_loss = 0.888\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.588\nstep 49/101, cls_loss = 1.013\nstep 99/101, cls_loss = 0.947\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.607\nstep 49/101, cls_loss = 0.839\nstep 99/101, cls_loss = 0.960\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.620\nstep 49/101, cls_loss = 0.963\nstep 99/101, cls_loss = 0.935\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.603\nstep 49/101, cls_loss = 0.887\nstep 99/101, cls_loss = 0.929\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.632\nstep 49/101, cls_loss = 0.830\nstep 99/101, cls_loss = 0.846\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.627\nstep 49/101, cls_loss = 0.730\nstep 99/101, cls_loss = 0.798\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.626\nstep 49/101, cls_loss = 0.680\nstep 99/101, cls_loss = 0.696\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.621\nstep 49/101, cls_loss = 0.685\nstep 99/101, cls_loss = 0.586\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.610\nstep 49/101, cls_loss = 0.552\nstep 99/101, cls_loss = 0.620\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.617\nstep 49/101, cls_loss = 0.518\nstep 99/101, cls_loss = 0.591\ncheckpoint has been created!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [00:17<00:00,  1.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.605\nstep 49/101, cls_loss = 0.582\nstep 99/101, cls_loss = 0.511\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"batch_size 64 epoch 20","metadata":{}},{"cell_type":"code","source":"modelz = AgeAlexNet(pretrainded=True,modelpath=\"/kaggle/working/checkpoint/epoch_19_iter_0.pth\").cuda()\nmodel=AgeClassify(modelz)\noptim=torch.optim.Adam(modelz.parameters(),lr=1e-5,betas=(0.5, 0.999))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:44:10.678542Z","iopub.execute_input":"2024-12-30T12:44:10.678830Z","iopub.status.idle":"2024-12-30T12:44:11.469091Z","shell.execute_reply.started":"2024-12-30T12:44:10.678806Z","shell.execute_reply":"2024-12-30T12:44:11.468442Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-bab3d8e81a13>:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_dict = torch.load(path)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\n\n#define\nmax_epoches = 10\nbatch_size = 64\nsave_interval = 400\nval_interval = 400\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nsaved_model_folder = '/kaggle/working/checkpoint2'\ncheck_dir(saved_model_folder)\n\nfor epoch in range(max_epoches):\n    for train_idx, (img,label) in enumerate(train_loader):\n        img=img.cuda()\n        label=label.cuda()\n\n        #train\n        optim.zero_grad()\n        model.train(img,label)\n        loss=model.loss\n        loss.backward()\n        optim.step()\n\n        if train_idx % 36 == 0:\n            print('step %d/%d, cls_loss = %.3f' % (train_idx, len(train_loader), loss))\n\n        # save the parameters at the end of each save interval\n        if (train_idx+1) % save_interval == 0:\n            model.save_model(dir=saved_model_folder,\n                              filename='epoch_%d_iter_%d.pth'%(epoch, train_idx))\n            print('checkpoint has been created!')\n            print(epoch)\n\n        #val step\n\n        if (train_idx+1) % val_interval == 0:\n            train_correct=0\n            train_total=0\n            with torch.no_grad():\n                for val_img,val_label in tqdm(test_loader):\n                    val_img=val_img.cuda()\n                    val_label=val_label.cuda()\n                    output=model.val(val_img)\n                    train_correct += (output == val_label).sum()\n                    train_total += val_img.size()[0]\n\n            print('validate has been finished!')\n            print('val_acc = %.3f' % (train_correct.cpu().numpy()/train_total))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:48:09.824051Z","iopub.execute_input":"2024-12-30T12:48:09.824392Z","iopub.status.idle":"2024-12-30T13:17:36.524046Z","shell.execute_reply.started":"2024-12-30T12:48:09.824363Z","shell.execute_reply":"2024-12-30T13:17:36.523385Z"}},"outputs":[{"name":"stdout","text":"step 0/403, cls_loss = 0.284\nstep 36/403, cls_loss = 0.458\nstep 72/403, cls_loss = 0.345\nstep 108/403, cls_loss = 0.401\nstep 144/403, cls_loss = 0.241\nstep 180/403, cls_loss = 0.220\nstep 216/403, cls_loss = 0.292\nstep 252/403, cls_loss = 0.341\nstep 288/403, cls_loss = 0.309\nstep 324/403, cls_loss = 0.309\nstep 360/403, cls_loss = 0.257\nstep 396/403, cls_loss = 0.247\ncheckpoint has been created!\n0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:18<00:00,  2.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.637\nstep 0/403, cls_loss = 0.291\nstep 36/403, cls_loss = 0.383\nstep 72/403, cls_loss = 0.280\nstep 108/403, cls_loss = 0.241\nstep 144/403, cls_loss = 0.376\nstep 180/403, cls_loss = 0.323\nstep 216/403, cls_loss = 0.140\nstep 252/403, cls_loss = 0.275\nstep 288/403, cls_loss = 0.445\nstep 324/403, cls_loss = 0.476\nstep 360/403, cls_loss = 0.555\nstep 396/403, cls_loss = 0.359\ncheckpoint has been created!\n1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.625\nstep 0/403, cls_loss = 0.300\nstep 36/403, cls_loss = 0.319\nstep 72/403, cls_loss = 0.403\nstep 108/403, cls_loss = 0.157\nstep 144/403, cls_loss = 0.285\nstep 180/403, cls_loss = 0.260\nstep 216/403, cls_loss = 0.436\nstep 252/403, cls_loss = 0.330\nstep 288/403, cls_loss = 0.395\nstep 324/403, cls_loss = 0.253\nstep 360/403, cls_loss = 0.317\nstep 396/403, cls_loss = 0.286\ncheckpoint has been created!\n2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:17<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.625\nstep 0/403, cls_loss = 0.263\nstep 36/403, cls_loss = 0.355\nstep 72/403, cls_loss = 0.231\nstep 108/403, cls_loss = 0.209\nstep 144/403, cls_loss = 0.295\nstep 180/403, cls_loss = 0.124\nstep 216/403, cls_loss = 0.225\nstep 252/403, cls_loss = 0.212\nstep 288/403, cls_loss = 0.375\nstep 324/403, cls_loss = 0.171\nstep 360/403, cls_loss = 0.374\nstep 396/403, cls_loss = 0.244\ncheckpoint has been created!\n3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.626\nstep 0/403, cls_loss = 0.251\nstep 36/403, cls_loss = 0.207\nstep 72/403, cls_loss = 0.390\nstep 108/403, cls_loss = 0.279\nstep 144/403, cls_loss = 0.294\nstep 180/403, cls_loss = 0.230\nstep 216/403, cls_loss = 0.197\nstep 252/403, cls_loss = 0.230\nstep 288/403, cls_loss = 0.360\nstep 324/403, cls_loss = 0.203\nstep 360/403, cls_loss = 0.268\nstep 396/403, cls_loss = 0.164\ncheckpoint has been created!\n4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.623\nstep 0/403, cls_loss = 0.179\nstep 36/403, cls_loss = 0.274\nstep 72/403, cls_loss = 0.280\nstep 108/403, cls_loss = 0.178\nstep 144/403, cls_loss = 0.138\nstep 180/403, cls_loss = 0.389\nstep 216/403, cls_loss = 0.340\nstep 252/403, cls_loss = 0.169\nstep 288/403, cls_loss = 0.304\nstep 324/403, cls_loss = 0.301\nstep 360/403, cls_loss = 0.325\nstep 396/403, cls_loss = 0.257\ncheckpoint has been created!\n5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.622\nstep 0/403, cls_loss = 0.156\nstep 36/403, cls_loss = 0.224\nstep 72/403, cls_loss = 0.305\nstep 108/403, cls_loss = 0.160\nstep 144/403, cls_loss = 0.297\nstep 180/403, cls_loss = 0.202\nstep 216/403, cls_loss = 0.296\nstep 252/403, cls_loss = 0.180\nstep 288/403, cls_loss = 0.177\nstep 324/403, cls_loss = 0.367\nstep 360/403, cls_loss = 0.162\nstep 396/403, cls_loss = 0.229\ncheckpoint has been created!\n6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.622\nstep 0/403, cls_loss = 0.196\nstep 36/403, cls_loss = 0.219\nstep 72/403, cls_loss = 0.237\nstep 108/403, cls_loss = 0.243\nstep 144/403, cls_loss = 0.219\nstep 180/403, cls_loss = 0.230\nstep 216/403, cls_loss = 0.151\nstep 252/403, cls_loss = 0.228\nstep 288/403, cls_loss = 0.113\nstep 324/403, cls_loss = 0.118\nstep 360/403, cls_loss = 0.313\nstep 396/403, cls_loss = 0.136\ncheckpoint has been created!\n7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:17<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.623\nstep 0/403, cls_loss = 0.139\nstep 36/403, cls_loss = 0.106\nstep 72/403, cls_loss = 0.194\nstep 108/403, cls_loss = 0.197\nstep 144/403, cls_loss = 0.209\nstep 180/403, cls_loss = 0.346\nstep 216/403, cls_loss = 0.255\nstep 252/403, cls_loss = 0.500\nstep 288/403, cls_loss = 0.144\nstep 324/403, cls_loss = 0.175\nstep 360/403, cls_loss = 0.120\nstep 396/403, cls_loss = 0.280\ncheckpoint has been created!\n8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.620\nstep 0/403, cls_loss = 0.206\nstep 36/403, cls_loss = 0.215\nstep 72/403, cls_loss = 0.186\nstep 108/403, cls_loss = 0.141\nstep 144/403, cls_loss = 0.197\nstep 180/403, cls_loss = 0.250\nstep 216/403, cls_loss = 0.274\nstep 252/403, cls_loss = 0.232\nstep 288/403, cls_loss = 0.215\nstep 324/403, cls_loss = 0.185\nstep 360/403, cls_loss = 0.236\nstep 396/403, cls_loss = 0.240\ncheckpoint has been created!\n9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:17<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"validate has been finished!\nval_acc = 0.618\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!mkdir /kaggle/working/model\n!cp /kaggle/working/checkpoint2/epoch_9_iter_399.pth /kaggle/working/model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:20:01.204094Z","iopub.execute_input":"2024-12-30T13:20:01.204459Z","iopub.status.idle":"2024-12-30T13:20:01.671575Z","shell.execute_reply.started":"2024-12-30T13:20:01.204428Z","shell.execute_reply":"2024-12-30T13:20:01.670406Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!zip -r model.zip /kaggle/working/model\n\nimport kagglehub\n\n# Replace with path to directory containing model files.\nLOCAL_MODEL_DIR = '/kaggle/working/model.zip'\n\nMODEL_SLUG = 'faceAlexnet_pretrain4emotion' # Replace with model slug.\n\n# Learn more about naming model variations at\n# https://www.kaggle.com/docs/models#name-model.\nVARIATION_SLUG = 'default' # Replace with variation slug.\n\nkagglehub.model_upload(\n  handle = f\"poongln/{MODEL_SLUG}/keras/{VARIATION_SLUG}\",\n  local_model_dir = LOCAL_MODEL_DIR,\n  version_notes = 'Update 2024-12-30')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:21:08.656103Z","iopub.execute_input":"2024-12-30T13:21:08.656546Z","iopub.status.idle":"2024-12-30T13:22:06.321817Z","shell.execute_reply.started":"2024-12-30T13:21:08.656507Z","shell.execute_reply":"2024-12-30T13:22:06.321054Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/model/ (stored 0%)\n  adding: kaggle/working/model/epoch_9_iter_399.pth (deflated 7%)\nUploading Model https://www.kaggle.com/models/poongln/faceAlexnet_pretrain4emotion/keras/default ...\nModel 'faceAlexnet_pretrain4emotion' does not exist or access is forbidden for user 'poongln'. Creating or handling Model...\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\nModel 'faceAlexnet_pretrain4emotion' Created.\nStarting upload for file /kaggle/working/model.zip\nWarning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 211M/211M [00:03<00:00, 68.8MB/s] ","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/model.zip (201MB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\nYour model instance has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/models/poongln/faceAlexnet_pretrain4emotion/keras/default\n","output_type":"stream"}],"execution_count":20}]}