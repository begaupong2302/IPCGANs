{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f8fe1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:30.542648Z",
     "iopub.status.busy": "2024-12-30T16:23:30.542379Z",
     "iopub.status.idle": "2024-12-30T16:23:34.696142Z",
     "shell.execute_reply": "2024-12-30T16:23:34.695485Z"
    },
    "papermill": {
     "duration": 4.161032,
     "end_time": "2024-12-30T16:23:34.697736",
     "exception": false,
     "start_time": "2024-12-30T16:23:30.536704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c2894",
   "metadata": {
    "papermill": {
     "duration": 0.002953,
     "end_time": "2024-12-30T16:23:34.704408",
     "exception": false,
     "start_time": "2024-12-30T16:23:34.701455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cfa9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:34.711262Z",
     "iopub.status.busy": "2024-12-30T16:23:34.710889Z",
     "iopub.status.idle": "2024-12-30T16:23:34.715936Z",
     "shell.execute_reply": "2024-12-30T16:23:34.715182Z"
    },
    "papermill": {
     "duration": 0.009806,
     "end_time": "2024-12-30T16:23:34.717162",
     "exception": false,
     "start_time": "2024-12-30T16:23:34.707356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def check_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "class Img_to_zero_center(object):\n",
    "    def __int__(self):\n",
    "        pass\n",
    "    def __call__(self, t_img):\n",
    "        '''\n",
    "        :param img:tensor be 0-1\n",
    "        :return:\n",
    "        '''\n",
    "        t_img=(t_img-0.5)*2\n",
    "        return t_img\n",
    "\n",
    "class Reverse_zero_center(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,t_img):\n",
    "        t_img=t_img/2+0.5\n",
    "        return t_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b1e2c",
   "metadata": {
    "papermill": {
     "duration": 0.002719,
     "end_time": "2024-12-30T16:23:34.722805",
     "exception": false,
     "start_time": "2024-12-30T16:23:34.720086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab85d17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:34.729590Z",
     "iopub.status.busy": "2024-12-30T16:23:34.729354Z",
     "iopub.status.idle": "2024-12-30T16:23:36.061462Z",
     "shell.execute_reply": "2024-12-30T16:23:36.060359Z"
    },
    "papermill": {
     "duration": 1.337194,
     "end_time": "2024-12-30T16:23:36.062927",
     "exception": false,
     "start_time": "2024-12-30T16:23:34.725733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 1, 3, 6, 4, 6, 2, 6, 2, 2, 0, 4, 0, 5, 1, 5, 0, 7, 0, 5, 7, 4, 0, 5,\n",
      "        7, 5, 2, 0, 5, 6, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "from random import shuffle\n",
    "\n",
    "class CACD(data.Dataset):\n",
    "    def __init__(self,split=\"train\",transforms=None, label_transforms=None):\n",
    "\n",
    "        self.split=split\n",
    "\n",
    "        #define label 128*128 for condition generate image\n",
    "        list_root = \"/kaggle/input/facial-expression-recognition/FER/lists\"\n",
    "        data_root = \"/kaggle/input/facial-expression-recognition/FER/FER\"\n",
    "        list_emotion = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "        \n",
    "        self.condition128=[]\n",
    "        full_one=np.ones((128,128),dtype=np.float32)\n",
    "        for i in range(8):\n",
    "            full_zero=np.zeros((128,128,8),dtype=np.float32)\n",
    "            full_zero[:,:,i]=full_one\n",
    "            self.condition128.append(full_zero)\n",
    "\n",
    "        # define label 64*64 for condition discriminate image\n",
    "        self.condition64 = []\n",
    "        full_one = np.ones((64, 64),dtype=np.float32)\n",
    "        for i in range(8):\n",
    "            full_zero = np.zeros((64, 64, 8),dtype=np.float32)\n",
    "            full_zero[:, :, i] = full_one\n",
    "            self.condition64.append(full_zero)\n",
    "\n",
    "        #define label_pairs\n",
    "        label_pair_root=\"/kaggle/input/facial-expression-recognition/train_label_pair.txt\"\n",
    "        with open(label_pair_root,'r') as f:\n",
    "            lines=f.readlines()\n",
    "        lines=[line.strip() for line in lines]\n",
    "        shuffle(lines)\n",
    "        self.label_pairs=[]\n",
    "        for line in lines:\n",
    "            label_pair=[]\n",
    "            items=line.split()\n",
    "            label_pair.append(int(items[0]))\n",
    "            label_pair.append(int(items[1]))\n",
    "            self.label_pairs.append(label_pair)\n",
    "\n",
    "        #define group_images\n",
    "        group_lists = [\n",
    "            os.path.join(list_root, 'train_emo_group_0.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_1.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_2.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_3.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_4.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_5.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_6.txt'),\n",
    "            os.path.join(list_root, 'train_emo_group_7.txt')\n",
    "        ]\n",
    "\n",
    "        self.label_group_images = []\n",
    "        for i in range(len(group_lists)):\n",
    "            with open(group_lists[i], 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip() for line in lines]\n",
    "            group_images = []\n",
    "            for l in lines:\n",
    "                items = l.split()\n",
    "                group_images.append(os.path.join(data_root,list_emotion[int(items[1])],items[0]))\n",
    "            self.label_group_images.append(group_images)\n",
    "\n",
    "        #define train.txt\n",
    "        if self.split == \"train\":\n",
    "            self.source_images = []#which use to aging transfer\n",
    "            with open(os.path.join(list_root, 'train.txt'), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip() for line in lines]\n",
    "            shuffle(lines)\n",
    "            for l in lines:\n",
    "                items = l.split()\n",
    "                self.source_images.append(os.path.join(data_root,list_emotion[int(items[1])],items[0]))\n",
    "        else:\n",
    "            self.source_images = []  # which use to aging transfer\n",
    "            with open(os.path.join(list_root, 'test.txt'), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip() for line in lines]\n",
    "            shuffle(lines)\n",
    "            for l in lines:\n",
    "                items = l.split()\n",
    "                self.source_images.append(os.path.join(data_root,list_emotion[int(items[1])],items[0]))\n",
    "\n",
    "        #define pointer\n",
    "        self.train_group_pointer=[0,0,0,0,0,0,0,0]\n",
    "        self.source_pointer=0\n",
    "        self.batch_size=32\n",
    "        self.transforms=transforms\n",
    "        self.label_transforms=label_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            pair_idx=idx//self.batch_size #a batch train the same pair\n",
    "            true_label=int(self.label_pairs[pair_idx][0])\n",
    "            fake_label=int(self.label_pairs[pair_idx][1])\n",
    "\n",
    "            true_label_128=self.condition128[true_label]\n",
    "            true_label_64=self.condition64[true_label]\n",
    "            fake_label_64=self.condition64[fake_label]\n",
    "\n",
    "            true_label_img=pil_loader(self.label_group_images[true_label][self.train_group_pointer[true_label]]).resize((128,128))\n",
    "            source_img=pil_loader(self.source_images[self.source_pointer])\n",
    "\n",
    "            source_img_227=source_img.resize((227,227))\n",
    "            source_img_128=source_img.resize((128,128))\n",
    "\n",
    "            if self.train_group_pointer[true_label]<len(self.label_group_images[true_label])-1:\n",
    "                self.train_group_pointer[true_label]+=1\n",
    "            else:\n",
    "                self.train_group_pointer[true_label]=0\n",
    "\n",
    "            if self.source_pointer<len(self.source_images)-1:\n",
    "                self.source_pointer+=1\n",
    "            else:\n",
    "                self.source_pointer=0\n",
    "\n",
    "            if self.transforms != None:\n",
    "                true_label_img=self.transforms(true_label_img)\n",
    "                source_img_227=self.transforms(source_img_227)\n",
    "                source_img_128=self.transforms(source_img_128)\n",
    "\n",
    "            if self.label_transforms != None:\n",
    "                true_label_128=self.label_transforms(true_label_128)\n",
    "                true_label_64=self.label_transforms(true_label_64)\n",
    "                fake_label_64=self.label_transforms(fake_label_64)\n",
    "            #source img 227 : use it to extract face feature\n",
    "            #source img 128 : use it to generate different age face -> then resize to (227,227) to extract feature, compile with source img 227\n",
    "            #ture_label_img : img in target age group -> use to train discriminator\n",
    "            #true_label_128 : use this condition to generate\n",
    "            #true_label_64 and fake_label_64 : use this condition to discrimination\n",
    "            #true_label : label\n",
    "\n",
    "            return source_img_227,source_img_128,true_label_img,true_label_128,true_label_64,fake_label_64, true_label\n",
    "        else:\n",
    "            source_img_128=pil_loader(self.source_images[idx]).resize((128,128))\n",
    "            if self.transforms != None:\n",
    "                source_img_128=self.transforms(source_img_128)\n",
    "            condition_128_tensor_li=[]\n",
    "            if self.label_transforms != None:\n",
    "                for condition in self.condition128:\n",
    "                    condition_128_tensor_li.append(self.label_transforms(condition).cuda())\n",
    "            return source_img_128.cuda(),condition_128_tensor_li\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.label_pairs)\n",
    "        else:\n",
    "            return len(self.source_images)\n",
    "\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "label_transforms=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "CACD_dataset=CACD(\"train\", transforms , label_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=CACD_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "for idx,(source_img_227,source_img_128,true_label_img,true_label_128,true_label_64,fake_label_64, true_label) in enumerate(train_loader):\n",
    "    print(true_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec883673",
   "metadata": {
    "papermill": {
     "duration": 0.002886,
     "end_time": "2024-12-30T16:23:36.069055",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.066169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**other architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44967e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:36.075723Z",
     "iopub.status.busy": "2024-12-30T16:23:36.075473Z",
     "iopub.status.idle": "2024-12-30T16:23:36.080955Z",
     "shell.execute_reply": "2024-12-30T16:23:36.080170Z"
    },
    "papermill": {
     "duration": 0.010161,
     "end_time": "2024-12-30T16:23:36.082122",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.071961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9909b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:36.088811Z",
     "iopub.status.busy": "2024-12-30T16:23:36.088588Z",
     "iopub.status.idle": "2024-12-30T16:23:36.100768Z",
     "shell.execute_reply": "2024-12-30T16:23:36.099975Z"
    },
    "papermill": {
     "duration": 0.016771,
     "end_time": "2024-12-30T16:23:36.101897",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.085126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.functional import pad\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "class _ConvNd(Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, transposed, output_padding, groups, bias):\n",
    "        super(_ConvNd, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                in_channels, out_channels // groups, *kernel_size))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        s += ')'\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "class Conv2d(_ConvNd):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return _conv2d_same_padding(input, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "# custom conv2d, because pytorch don't have \"padding='same'\" option.\n",
    "def _conv2d_same_padding(input, weight, bias=None, stride=(1,1), padding=1, dilation=(1,1), groups=1):\n",
    "\n",
    "    input_rows = input.size(2)\n",
    "    filter_rows = weight.size(2)\n",
    "    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n",
    "    out_rows = (input_rows + stride[0] - 1) // stride[0]\n",
    "    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n",
    "                  input_rows)\n",
    "    padding_rows = max(0, (out_rows - 1) * stride[0] +\n",
    "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
    "    rows_odd = (padding_rows % 2 != 0)\n",
    "    padding_cols = max(0, (out_rows - 1) * stride[0] +\n",
    "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
    "    cols_odd = (padding_rows % 2 != 0)\n",
    "\n",
    "    if rows_odd or cols_odd:\n",
    "        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n",
    "\n",
    "    return F.conv2d(input, weight, bias, stride,\n",
    "                  padding=(padding_rows // 2, padding_cols // 2),\n",
    "                  dilation=dilation, groups=groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b00a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:36.108656Z",
     "iopub.status.busy": "2024-12-30T16:23:36.108439Z",
     "iopub.status.idle": "2024-12-30T16:23:36.120134Z",
     "shell.execute_reply": "2024-12-30T16:23:36.119358Z"
    },
    "papermill": {
     "duration": 0.016284,
     "end_time": "2024-12-30T16:23:36.121306",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.105022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AgeAlexNet(nn.Module):\n",
    "    def __init__(self,pretrainded=False,modelpath=None):\n",
    "        super(AgeAlexNet, self).__init__()\n",
    "        assert pretrainded is False or modelpath is not None,\"pretrain model need to be specified\"\n",
    "        self.features = nn.Sequential(\n",
    "            Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LocalResponseNorm(2,2e-5,0.75),\n",
    "\n",
    "            Conv2d(96, 256, kernel_size=5, stride=1,groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LocalResponseNorm(2, 2e-5, 0.75),\n",
    "\n",
    "            Conv2d(256, 384, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            Conv2d(384, 384, kernel_size=3,stride=1,groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            Conv2d(384, 256, kernel_size=3,stride=1,groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.age_classifier=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 8),\n",
    "        )\n",
    "        if pretrainded is True:\n",
    "            self.load_pretrained_params(modelpath)\n",
    "\n",
    "        self.Conv3_feature_module=nn.Sequential()\n",
    "        self.Conv4_feature_module=nn.Sequential()\n",
    "        self.Conv5_feature_module=nn.Sequential()\n",
    "        self.Pool5_feature_module=nn.Sequential()\n",
    "        for x in range(10):\n",
    "            self.Conv3_feature_module.add_module(str(x), self.features[x])\n",
    "        for x in range(10,12):\n",
    "            self.Conv4_feature_module.add_module(str(x),self.features[x])\n",
    "        for x in range(12,14):\n",
    "            self.Conv5_feature_module.add_module(str(x),self.features[x])\n",
    "        for x in range(14,15):\n",
    "            self.Pool5_feature_module.add_module(str(x),self.features[x])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv3_feature=self.Conv3_feature_module(x)\n",
    "        self.conv4_feature=self.Conv4_feature_module(self.conv3_feature)\n",
    "        self.conv5_feature=self.Conv5_feature_module(self.conv4_feature)\n",
    "        pool5_feature=self.Pool5_feature_module(self.conv5_feature)\n",
    "        self.pool5_feature=pool5_feature\n",
    "        flattened = pool5_feature.view(pool5_feature.size(0), -1)\n",
    "        age_logit = self.age_classifier(flattened)\n",
    "        return age_logit\n",
    "\n",
    "    def load_pretrained_params(self,path):\n",
    "        # step1: load pretrained model\n",
    "        pretrained_dict = torch.load(path)\n",
    "        # step2: get model state_dict\n",
    "        model_dict = self.state_dict()\n",
    "        # step3: remove pretrained_dict params which is not in model_dict\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # step4: update model_dict using pretrained_dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # step5: update model using model_dict\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "class AgeClassify:\n",
    "    def __init__(self):\n",
    "        #step 1:define model\n",
    "        self.model=AgeAlexNet(pretrainded=False).cuda()\n",
    "        #step 2:define optimizer\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=1e-4,betas=(0.5, 0.999))\n",
    "        #step 3:define loss\n",
    "        self.criterion=nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    def train(self,input,label):\n",
    "        self.model.train()\n",
    "        output=self.model(input)\n",
    "        self.loss=self.criterion(output,label)\n",
    "\n",
    "    def val(self,input):\n",
    "        self.model.eval()\n",
    "        output=F.softmax(self.model(input),dim=1).max(1)[1]\n",
    "        return output\n",
    "\n",
    "    def save_model(self,dir,filename):\n",
    "        torch.save(self.model.state_dict(),os.path.join(dir,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cba309",
   "metadata": {
    "papermill": {
     "duration": 0.002715,
     "end_time": "2024-12-30T16:23:36.126956",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.124241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**IPCGANs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4aec61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:36.134888Z",
     "iopub.status.busy": "2024-12-30T16:23:36.134575Z",
     "iopub.status.idle": "2024-12-30T16:23:36.160018Z",
     "shell.execute_reply": "2024-12-30T16:23:36.159362Z"
    },
    "papermill": {
     "duration": 0.031339,
     "end_time": "2024-12-30T16:23:36.161200",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.129861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.conv1 = Conv2d(3, 64, kernel_size=4, stride=2)\n",
    "        self.conv2 = Conv2d(72, 128, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128, eps=0.001, track_running_stats=True)\n",
    "        self.conv3 = Conv2d(128, 256, kernel_size=4, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256, eps=0.001, track_running_stats=True)\n",
    "        self.conv4 = Conv2d(256, 512, kernel_size=4, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512, eps=0.001, track_running_stats=True)\n",
    "        self.conv5 = Conv2d(512, 512, kernel_size=4, stride=2)\n",
    "\n",
    "    def forward(self, x,condition):\n",
    "        x = self.lrelu(self.conv1(x))\n",
    "        x=torch.cat((x,condition),1)\n",
    "        x = self.lrelu(self.bn2(self.conv2(x)))\n",
    "        x = self.lrelu(self.bn3(self.conv3(x)))\n",
    "        x = self.lrelu(self.bn4(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = Conv2d(11, 32, kernel_size=7, stride=1)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv3 = Conv2d(64, 128, kernel_size=3, stride=2)\n",
    "        self.conv4 = Conv2d(32, 3, kernel_size=7, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32, eps=0.001, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm2d(64, eps=0.001, track_running_stats=True)\n",
    "        self.bn3 = nn.BatchNorm2d(128, eps=0.001, track_running_stats=True)\n",
    "        self.bn4 = nn.BatchNorm2d(64, eps=0.001, track_running_stats=True)\n",
    "        self.bn5 = nn.BatchNorm2d(32, eps=0.001, track_running_stats=True)\n",
    "        self.repeat_blocks=self._make_repeat_blocks(BasicBlock(128,128),6)\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=0,output_padding=1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "\n",
    "    def _make_repeat_blocks(self,block,repeat_times):\n",
    "        layers=[]\n",
    "        for i in range(repeat_times):\n",
    "            layers.append(block)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x,condition=None):\n",
    "        if condition is not None:\n",
    "            x=torch.cat((x,condition),1)\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.repeat_blocks(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu(self.bn4(x))\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu(self.bn5(x))\n",
    "        x = self.tanh(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "class IPCGANs:\n",
    "    def __init__(self,lr=0.01,age_classifier_path=None,gan_loss_weight=75,feature_loss_weight=0.5e-4,age_loss_weight=30):\n",
    "\n",
    "        self.d_lr=lr\n",
    "        self.g_lr=lr\n",
    "\n",
    "        self.generator=Generator().cuda()\n",
    "        self.discriminator=PatchDiscriminator().cuda()\n",
    "        if age_classifier_path != None:\n",
    "            self.age_classifier=AgeAlexNet(pretrainded=True,modelpath=age_classifier_path).cuda()\n",
    "        else:\n",
    "            self.age_classifier = AgeAlexNet(pretrainded=False).cuda()\n",
    "        self.MSEloss=nn.MSELoss().cuda()\n",
    "        self.CrossEntropyLoss=nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.gan_loss_weight=gan_loss_weight\n",
    "        self.feature_loss_weight = feature_loss_weight\n",
    "        self.age_loss_weight=age_loss_weight\n",
    "\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(),self.d_lr,betas=(0.5,0.99))\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), self.g_lr, betas=(0.5, 0.99))\n",
    "\n",
    "    def save_model(self,dir,filename):\n",
    "        torch.save(self.generator.state_dict(),os.path.join(dir,\"g\"+filename))\n",
    "        torch.save(self.discriminator.state_dict(),os.path.join(dir,\"d\"+filename))\n",
    "\n",
    "    def load_generator_state_dict(self,state_dict):\n",
    "        pretrained_dict = state_dict\n",
    "        # step2: get model state_dict\n",
    "        model_dict = self.generator.state_dict()\n",
    "        # step3: remove pretrained_dict params which is not in model_dict\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # step4: update model_dict using pretrained_dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # step5: update model using model_dict\n",
    "        self.generator.load_state_dict(model_dict)\n",
    "\n",
    "    def test_generate(self,source_img_128,condition):\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            generate_image=self.generator(source_img_128,condition)\n",
    "        return generate_image\n",
    "\n",
    "    def cuda(self):\n",
    "        self.generator=self.generator.cuda()\n",
    "\n",
    "    def train(self,source_img_227,source_img_128,true_label_img,true_label_128,true_label_64,fake_label_64, age_label):\n",
    "        '''\n",
    "\n",
    "        :param source_img_227: use this img to extract conv5 feature\n",
    "        :param source_img_128: use this img to generate face in target age\n",
    "        :param true_label_img:\n",
    "        :param true_label_128:\n",
    "        :param true_label_64:\n",
    "        :param fake_label_64:\n",
    "        :param age_label:\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        ###################################gan_loss###############################\n",
    "        self.g_source=self.generator(source_img_128,condition=true_label_128)\n",
    "\n",
    "        #real img, right age label\n",
    "        #logit means prob which hasn't been normalized\n",
    "\n",
    "        #d1 logit ,discriminator 1 means true,0 means false.\n",
    "        d1_logit=self.discriminator(true_label_img,condition=true_label_64)\n",
    "\n",
    "        d1_real_loss=self.MSEloss(d1_logit,torch.ones((d1_logit.size())).cuda())\n",
    "\n",
    "        #real img, false label\n",
    "        d2_logit=self.discriminator(true_label_img,condition=fake_label_64)\n",
    "        d2_fake_loss=self.MSEloss(d2_logit,torch.zeros((d1_logit.size())).cuda())\n",
    "\n",
    "        #fake img,real label\n",
    "        d3_logit=self.discriminator(self.g_source,condition=true_label_64)\n",
    "        d3_fake_loss=self.MSEloss(d3_logit,torch.zeros((d1_logit.size())).cuda())#use this for discriminator\n",
    "        d3_real_loss=self.MSEloss(d3_logit,torch.ones((d1_logit.size())).cuda())#use this for genrator\n",
    "\n",
    "        self.d_loss=(1./2 * (d1_real_loss + 1. / 2 * (d2_fake_loss + d3_fake_loss))) * self.gan_loss_weight\n",
    "        g_loss=(1./2*d3_real_loss)*self.gan_loss_weight\n",
    "\n",
    "\n",
    "        ################################feature_loss#############################\n",
    "\n",
    "        self.age_classifier(source_img_227)\n",
    "        source_feature=self.age_classifier.conv5_feature\n",
    "\n",
    "        generate_img_227 = F.interpolate(self.g_source, (227, 227), mode=\"bilinear\", align_corners=True)\n",
    "        generate_img_227 = Img_to_zero_center()(generate_img_227)\n",
    "\n",
    "        self.age_classifier(generate_img_227)\n",
    "        generate_feature =self.age_classifier.conv5_feature\n",
    "        self.feature_loss=self.MSEloss(source_feature,generate_feature)\n",
    "\n",
    "        ################################age_cls_loss##############################\n",
    "\n",
    "\n",
    "\n",
    "        age_logit=self.age_classifier(generate_img_227)\n",
    "        self.age_loss=self.CrossEntropyLoss(age_logit,age_label)\n",
    "\n",
    "        self.g_loss=self.age_loss+g_loss+self.feature_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7949b",
   "metadata": {
    "papermill": {
     "duration": 0.002853,
     "end_time": "2024-12-30T16:23:36.167136",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.164283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**trainning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7f5528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T16:23:36.175243Z",
     "iopub.status.busy": "2024-12-30T16:23:36.175005Z",
     "iopub.status.idle": "2024-12-31T00:24:51.540254Z",
     "shell.execute_reply": "2024-12-31T00:24:51.539441Z"
    },
    "papermill": {
     "duration": 28875.37174,
     "end_time": "2024-12-31T00:24:51.541860",
     "exception": false,
     "start_time": "2024-12-30T16:23:36.170120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-aea4be959472>:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "step 350/1407, g_loss = 11.927, d_loss = 21.102\n",
      "step 700/1407, g_loss = 15.136, d_loss = 20.022\n",
      "step 1050/1407, g_loss = 13.517, d_loss = 15.830\n",
      "step 1400/1407, g_loss = 9.753, d_loss = 14.713\n",
      "epoch: 1\n",
      "step 350/1407, g_loss = 18.512, d_loss = 15.340\n",
      "step 700/1407, g_loss = 10.170, d_loss = 15.167\n",
      "step 1050/1407, g_loss = 15.369, d_loss = 17.617\n",
      "step 1400/1407, g_loss = 12.964, d_loss = 14.253\n",
      "epoch: 2\n",
      "step 350/1407, g_loss = 15.737, d_loss = 11.771\n",
      "step 700/1407, g_loss = 12.794, d_loss = 10.630\n",
      "step 1050/1407, g_loss = 12.577, d_loss = 11.604\n",
      "step 1400/1407, g_loss = 15.261, d_loss = 9.269\n",
      "epoch: 3\n",
      "step 350/1407, g_loss = 14.145, d_loss = 14.470\n",
      "step 700/1407, g_loss = 11.182, d_loss = 12.252\n",
      "step 1050/1407, g_loss = 11.640, d_loss = 11.066\n",
      "step 1400/1407, g_loss = 18.102, d_loss = 13.842\n",
      "epoch: 4\n",
      "step 350/1407, g_loss = 21.631, d_loss = 10.184\n",
      "step 700/1407, g_loss = 19.375, d_loss = 16.977\n",
      "step 1050/1407, g_loss = 12.398, d_loss = 12.048\n",
      "step 1400/1407, g_loss = 15.415, d_loss = 20.681\n",
      "epoch: 5\n",
      "step 350/1407, g_loss = 10.583, d_loss = 11.360\n",
      "step 700/1407, g_loss = 14.913, d_loss = 9.901\n",
      "step 1050/1407, g_loss = 14.287, d_loss = 20.286\n",
      "step 1400/1407, g_loss = 14.015, d_loss = 10.544\n",
      "epoch: 6\n",
      "step 350/1407, g_loss = 26.923, d_loss = 13.183\n",
      "step 700/1407, g_loss = 20.440, d_loss = 14.047\n",
      "step 1050/1407, g_loss = 18.269, d_loss = 11.480\n",
      "step 1400/1407, g_loss = 13.372, d_loss = 9.918\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:54<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 7\n",
      "step 350/1407, g_loss = 30.858, d_loss = 9.376\n",
      "step 700/1407, g_loss = 16.465, d_loss = 9.584\n",
      "step 1050/1407, g_loss = 17.304, d_loss = 8.596\n",
      "step 1400/1407, g_loss = 15.926, d_loss = 9.467\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:40<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 8\n",
      "step 350/1407, g_loss = 15.651, d_loss = 13.310\n",
      "step 700/1407, g_loss = 19.073, d_loss = 17.230\n",
      "step 1050/1407, g_loss = 21.279, d_loss = 8.423\n",
      "step 1400/1407, g_loss = 20.034, d_loss = 7.843\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 9\n",
      "step 350/1407, g_loss = 31.032, d_loss = 9.744\n",
      "step 700/1407, g_loss = 21.404, d_loss = 6.829\n",
      "step 1050/1407, g_loss = 17.472, d_loss = 7.795\n",
      "step 1400/1407, g_loss = 16.462, d_loss = 8.851\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 10\n",
      "step 350/1407, g_loss = 20.683, d_loss = 8.764\n",
      "step 700/1407, g_loss = 20.633, d_loss = 7.593\n",
      "step 1050/1407, g_loss = 24.099, d_loss = 9.901\n",
      "step 1400/1407, g_loss = 33.028, d_loss = 9.536\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:41<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 11\n",
      "step 350/1407, g_loss = 25.890, d_loss = 9.720\n",
      "step 700/1407, g_loss = 14.686, d_loss = 8.403\n",
      "step 1050/1407, g_loss = 21.705, d_loss = 9.130\n",
      "step 1400/1407, g_loss = 20.768, d_loss = 8.203\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 12\n",
      "step 350/1407, g_loss = 22.250, d_loss = 8.826\n",
      "step 700/1407, g_loss = 12.469, d_loss = 15.089\n",
      "step 1050/1407, g_loss = 22.218, d_loss = 6.979\n",
      "step 1400/1407, g_loss = 28.212, d_loss = 6.735\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 13\n",
      "step 350/1407, g_loss = 21.539, d_loss = 6.586\n",
      "step 700/1407, g_loss = 16.102, d_loss = 7.263\n",
      "step 1050/1407, g_loss = 16.392, d_loss = 10.250\n",
      "step 1400/1407, g_loss = 32.822, d_loss = 5.230\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 14\n",
      "step 350/1407, g_loss = 31.939, d_loss = 16.523\n",
      "step 700/1407, g_loss = 22.804, d_loss = 8.114\n",
      "step 1050/1407, g_loss = 22.154, d_loss = 9.183\n",
      "step 1400/1407, g_loss = 17.727, d_loss = 7.500\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 15\n",
      "step 350/1407, g_loss = 23.350, d_loss = 7.073\n",
      "step 700/1407, g_loss = 21.878, d_loss = 6.460\n",
      "step 1050/1407, g_loss = 28.662, d_loss = 5.096\n",
      "step 1400/1407, g_loss = 19.550, d_loss = 7.541\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:40<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 16\n",
      "step 350/1407, g_loss = 50.890, d_loss = 11.416\n",
      "step 700/1407, g_loss = 39.072, d_loss = 10.063\n",
      "step 1050/1407, g_loss = 21.419, d_loss = 11.600\n",
      "step 1400/1407, g_loss = 20.263, d_loss = 7.143\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 17\n",
      "step 350/1407, g_loss = 20.124, d_loss = 6.830\n",
      "step 700/1407, g_loss = 21.149, d_loss = 7.344\n",
      "step 1050/1407, g_loss = 28.719, d_loss = 5.049\n",
      "step 1400/1407, g_loss = 20.414, d_loss = 8.315\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:40<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 18\n",
      "step 350/1407, g_loss = 28.317, d_loss = 8.610\n",
      "step 700/1407, g_loss = 32.049, d_loss = 4.434\n",
      "step 1050/1407, g_loss = 30.435, d_loss = 9.917\n",
      "step 1400/1407, g_loss = 23.238, d_loss = 6.532\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 19\n",
      "step 350/1407, g_loss = 29.137, d_loss = 5.061\n",
      "step 700/1407, g_loss = 24.830, d_loss = 5.546\n",
      "step 1050/1407, g_loss = 27.038, d_loss = 4.281\n",
      "step 1400/1407, g_loss = 23.960, d_loss = 6.780\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 20\n",
      "step 350/1407, g_loss = 12.442, d_loss = 9.546\n",
      "step 700/1407, g_loss = 24.891, d_loss = 8.440\n",
      "step 1050/1407, g_loss = 18.206, d_loss = 9.037\n",
      "step 1400/1407, g_loss = 23.893, d_loss = 7.164\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 21\n",
      "step 350/1407, g_loss = 32.354, d_loss = 4.370\n",
      "step 700/1407, g_loss = 34.829, d_loss = 6.161\n",
      "step 1050/1407, g_loss = 28.194, d_loss = 5.846\n",
      "step 1400/1407, g_loss = 35.226, d_loss = 7.520\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 22\n",
      "step 350/1407, g_loss = 29.655, d_loss = 4.624\n",
      "step 700/1407, g_loss = 33.771, d_loss = 8.674\n",
      "step 1050/1407, g_loss = 27.979, d_loss = 4.897\n",
      "step 1400/1407, g_loss = 32.377, d_loss = 12.838\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:39<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 23\n",
      "step 350/1407, g_loss = 20.877, d_loss = 6.266\n",
      "step 700/1407, g_loss = 30.473, d_loss = 22.114\n",
      "step 1050/1407, g_loss = 23.724, d_loss = 6.810\n",
      "step 1400/1407, g_loss = 36.563, d_loss = 3.128\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 24\n",
      "step 350/1407, g_loss = 21.429, d_loss = 18.655\n",
      "step 700/1407, g_loss = 25.539, d_loss = 8.960\n",
      "step 1050/1407, g_loss = 29.870, d_loss = 5.894\n",
      "step 1400/1407, g_loss = 32.518, d_loss = 4.460\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 25\n",
      "step 350/1407, g_loss = 23.391, d_loss = 6.122\n",
      "step 700/1407, g_loss = 18.150, d_loss = 7.880\n",
      "step 1050/1407, g_loss = 22.412, d_loss = 9.252\n",
      "step 1400/1407, g_loss = 36.385, d_loss = 3.575\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 26\n",
      "step 350/1407, g_loss = 25.795, d_loss = 6.416\n",
      "step 700/1407, g_loss = 32.743, d_loss = 2.920\n",
      "step 1050/1407, g_loss = 35.279, d_loss = 5.039\n",
      "step 1400/1407, g_loss = 23.462, d_loss = 7.246\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 27\n",
      "step 350/1407, g_loss = 36.630, d_loss = 11.488\n",
      "step 700/1407, g_loss = 34.978, d_loss = 5.205\n",
      "step 1050/1407, g_loss = 29.551, d_loss = 19.648\n",
      "step 1400/1407, g_loss = 25.473, d_loss = 10.301\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 28\n",
      "step 350/1407, g_loss = 35.475, d_loss = 2.765\n",
      "step 700/1407, g_loss = 22.663, d_loss = 6.516\n",
      "step 1050/1407, g_loss = 23.561, d_loss = 6.155\n",
      "step 1400/1407, g_loss = 24.737, d_loss = 8.114\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:37<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n",
      "epoch: 29\n",
      "step 350/1407, g_loss = 38.263, d_loss = 10.591\n",
      "step 700/1407, g_loss = 39.916, d_loss = 5.043\n",
      "step 1050/1407, g_loss = 35.397, d_loss = 4.774\n",
      "step 1400/1407, g_loss = 32.916, d_loss = 4.749\n",
      "checkpoint has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:38<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation image has been created!\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "max_epoches = 30\n",
    "val_interval = 1400 #Number of steps to validate\n",
    "save_interval = 1400 #Number of batches to save model\n",
    "\n",
    "d_iters = 1 \n",
    "g_iters = 1 \n",
    "\n",
    "#model\n",
    "gan_loss_weight = 75\n",
    "feature_loss_weight = 0.5e-4\n",
    "age_loss_weight = 30\n",
    "age_groups = 5\n",
    "age_classifier_path = \"/kaggle/input/facealexnet_pretrain4emotion/keras/default/1/kaggle/working/model/epoch_9_iter_399.pth\"\n",
    "\n",
    "#data, io\n",
    "checkpoint = \"/kaggle/working/checkpoint/\"\n",
    "saved_model_folder = \"/kaggle/working/checkpoint/saved_parameters/\"\n",
    "saved_validation_folder = \"/kaggle/working/checkpoint/validation/\"\n",
    "\n",
    "#check_dir\n",
    "check_dir(checkpoint)\n",
    "check_dir(saved_model_folder)\n",
    "check_dir(saved_validation_folder)\n",
    "\n",
    "def main():\n",
    "    print(\"Start to train:\\n\")\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        Img_to_zero_center()\n",
    "    ])\n",
    "    label_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CACD(\"train\",transforms, label_transforms)\n",
    "    test_dataset = CACD(\"test\", transforms, label_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model=IPCGANs(lr=learning_rate,age_classifier_path=age_classifier_path,gan_loss_weight=gan_loss_weight,feature_loss_weight=feature_loss_weight,age_loss_weight=age_loss_weight)\n",
    "    d_optim=model.d_optim\n",
    "    g_optim=model.g_optim\n",
    "\n",
    "    for epoch in range(max_epoches):\n",
    "        epochp = \"epoch: \" + str(epoch)\n",
    "        print(epochp)\n",
    "        for idx, (source_img_227,source_img_128,true_label_img,true_label_128,true_label_64,fake_label_64, true_label) in enumerate(train_loader,1):\n",
    "\n",
    "            running_d_loss=None\n",
    "            running_g_loss=None\n",
    "            n_iter = epoch * len(train_loader) + idx\n",
    "\n",
    "\n",
    "            #mv to gpu\n",
    "            source_img_227=source_img_227.cuda()\n",
    "            source_img_128=source_img_128.cuda()\n",
    "            true_label_img=true_label_img.cuda()\n",
    "            true_label_128=true_label_128.cuda()\n",
    "            true_label_64=true_label_64.cuda()\n",
    "            fake_label_64=fake_label_64.cuda()\n",
    "            true_label=true_label.cuda()\n",
    "\n",
    "            #train discriminator\n",
    "            for d_iter in range(d_iters):\n",
    "                #d_lr_scheduler.step()\n",
    "                d_optim.zero_grad()\n",
    "                model.train(\n",
    "                    source_img_227=source_img_227,\n",
    "                    source_img_128=source_img_128,\n",
    "                    true_label_img=true_label_img,\n",
    "                    true_label_128=true_label_128,\n",
    "                    true_label_64=true_label_64,\n",
    "                    fake_label_64=fake_label_64,\n",
    "                    age_label=true_label\n",
    "                )\n",
    "                d_loss=model.d_loss\n",
    "                running_d_loss=d_loss\n",
    "                d_loss.backward()\n",
    "                d_optim.step()\n",
    "\n",
    "            #train generator\n",
    "            for g_iter in range(g_iters):\n",
    "                #g_lr_scheduler.step()\n",
    "                g_optim.zero_grad()\n",
    "                model.train(\n",
    "                    source_img_227=source_img_227,\n",
    "                    source_img_128=source_img_128,\n",
    "                    true_label_img=true_label_img,\n",
    "                    true_label_128=true_label_128,\n",
    "                    true_label_64=true_label_64,\n",
    "                    fake_label_64=fake_label_64,\n",
    "                    age_label=true_label\n",
    "                )\n",
    "                g_loss = model.g_loss\n",
    "                running_g_loss=g_loss\n",
    "                g_loss.backward()\n",
    "                g_optim.step()\n",
    "            if idx % 350 == 0:\n",
    "                print('step %d/%d, g_loss = %.3f, d_loss = %.3f' %(idx, len(train_loader),running_g_loss,running_d_loss))\n",
    "\n",
    "            # save the parameters at the end of each save interval\n",
    "            if idx % save_interval == 0 and epoch > 5:\n",
    "                model.save_model(dir=saved_model_folder,\n",
    "                                 filename='epoch_%d_iter_%d.pth'%(epoch, idx))\n",
    "                print('checkpoint has been created!')\n",
    "\n",
    "            #val step\n",
    "            if idx % val_interval == 0 and epoch > 5:\n",
    "                save_dir = os.path.join(saved_validation_folder, \"epoch_%d\" % epoch, \"idx_%d\" % idx)\n",
    "                check_dir(save_dir)\n",
    "                for val_idx, (source_img_128, true_label_128) in enumerate(tqdm(test_loader)):\n",
    "                    save_image(Reverse_zero_center()(source_img_128),os.path.join(save_dir,\"batch_%d_source.jpg\"%(val_idx)))\n",
    "\n",
    "                    pic_list = []\n",
    "                    pic_list.append(source_img_128)\n",
    "                    for age in range(age_groups):\n",
    "                        img = model.test_generate(source_img_128, true_label_128[age])\n",
    "                        save_image(Reverse_zero_center()(img),os.path.join(save_dir,\"batch_%d_age_group_%d.jpg\"%(val_idx,age)))\n",
    "                print('validation image has been created!')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d9e397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T00:24:51.760517Z",
     "iopub.status.busy": "2024-12-31T00:24:51.760178Z",
     "iopub.status.idle": "2024-12-31T00:24:51.766007Z",
     "shell.execute_reply": "2024-12-31T00:24:51.765063Z"
    },
    "papermill": {
     "duration": 0.116189,
     "end_time": "2024-12-31T00:24:51.767564",
     "exception": false,
     "start_time": "2024-12-31T00:24:51.651375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport subprocess\\nfrom IPython.display import FileLink, display\\n\\ndef download_file(path, download_file_name):\\n    os.chdir(\\'/kaggle/working/\\')\\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\\n    command = f\"zip {zip_name} {path} -r\"\\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\\n    if result.returncode != 0:\\n        print(\"Unable to run zip command!\")\\n        print(result.stderr)\\n        return\\n    display(FileLink(f\\'{download_file_name}.zip\\'))\\n\\ndownload_file(\\'/kaggle/working/checkpoint\\', \\'out\\')\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "def download_file(path, download_file_name):\n",
    "    os.chdir('/kaggle/working/')\n",
    "    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
    "    command = f\"zip {zip_name} {path} -r\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Unable to run zip command!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "    display(FileLink(f'{download_file_name}.zip'))\n",
    "\n",
    "download_file('/kaggle/working/checkpoint', 'out')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6398407,
     "sourceId": 10335326,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205001,
     "modelInstanceId": 182794,
     "sourceId": 214427,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28885.030726,
   "end_time": "2024-12-31T00:24:53.441842",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-30T16:23:28.411116",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
